3. Experiment
Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema
et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al.,
2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al.,
2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI,
11
2024d), Aider 1 , LiveCodeBench (Jain et al., 2024) (2024-08 – 2025-01), Codeforces 2 , Chinese
National High School Mathematics Olympiad (CNMO 2024)3 , and American Invitational Math-
ematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we
also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we
adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li
et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we
only feed the final summary to evaluation to avoid the length bias. For distilled models, we
report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and
LiveCodeBench.
Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as
MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-
evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a
zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts
are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot
may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation
protocols with default prompts provided by their creators. For code and math benchmarks, the
HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++,
C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated
using CoT format, with data collected between August 2024 and January 2025. The Codeforces
dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases,
after which the expected ratings and percentages of competitors are calculated. SWE-Bench
verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related
benchmarks are measured using a "diff" format. DeepSeek-R1 outputs are capped at a maximum
of 32,768 tokens for each benchmark.
Baselines We conduct comprehensive evaluations against several strong baselines, including
DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.
Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor-
mance based on official reports. For distilled models, we also compare the open-source model
QwQ-32B-Preview (Qwen, 2024a).
Evaluation Setup We set the maximum generation length to 32,768 tokens for the models.
We found that using greedy decoding to evaluate long-output reasoning models results in
higher repetition rates and significant variability across different checkpoints. Therefore, we
default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature.
Specifically, we use a sampling temperature of 0.6 and a top- p value of 0.95 to generate k
responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1
is then calculated as
k
1 ∑︁
pass@1 =
 pi ,
k
i =1
where p i denotes the correctness of the i-th response. This method provides more reliable
performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang
et al., 2022) using 64 samples, denoted as cons@64.
1 https://aider.chat
2 https://codeforces.com
3https://www.cms.org.cn/Home/comp/comp/cid/12.html
12
3.1. DeepSeek-R1 Evaluation
Benchmark (Metric)
Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek
Sonnet-1022 0513
 V3
 o1-mini o1-1217
 R1
Architecture
 -
 -
 MoE
 -
 -
 MoE
# Activated Params
 -
 -
 37B
 -
 -
 37B
# Total Params
 -
 -
 671B
 -
 -
 671B
MMLU (Pass@1)
 88.3
 87.2
 88.5
 85.2
 91.8
 90.8
MMLU-Redux (EM)
 88.9
 88.0
 89.1
 86.7
 -
 92.9
MMLU-Pro (EM)
 78.0
 72.6
 75.9
 80.3
 -
 84.0
DROP (3-shot F1)
 88.3
 83.7
 91.6
 83.9
 90.2
 92.2
IF-Eval (Prompt Strict)
 86.5
 84.3
 86.1
 84.8
 -
 83.3
English
GPQA Diamond (Pass@1)
 65.0
 49.9
 59.1
 60.0
 75.7
 71.5
SimpleQA (Correct)
 28.4
 38.2
 24.9
 7.0
 47.0
 30.1
FRAMES (Acc.)
 72.5
 80.5
 73.3
 76.9
 -
 82.5
AlpacaEval2.0 (LC-winrate)
 52.0
 51.1
 70.0
 57.8
 -
 87.6
ArenaHard (GPT-4-1106)
 85.2
 80.4
 85.5
 92.0
 -
 92.3
LiveCodeBench (Pass@1-COT)
 38.9
 32.9
 36.2
 53.8
 63.4
 65.9
Codeforces (Percentile)
 20.3
 23.6
 58.7
 93.4
 96.6
 96.3
Code
Codeforces (Rating)
 717
 759
 1134
 1820
 2061
 2029
SWE Verified (Resolved)
 50.8
 38.8
 42.0
 41.6
 48.9
 49.2
Aider-Polyglot (Acc.)
 45.3
 16.0
 49.6
 32.9
 61.7
 53.3
Math
AIME 2024 (Pass@1)
MATH-500 (Pass@1)
CNMO 2024 (Pass@1)
CLUEWSC (EM)
Chinese C-Eval (EM)
C-SimpleQA (Correct)
16.0
 9.3
 39.2
 63.6
 79.2
 79.8
78.3
 74.6
 90.2
 90.0
 96.4
 97.3
13.1
 10.8
 43.2
 67.6
 -
 78.8
85.4
 87.9
 90.9
 89.9
 -
 92.8
76.7
 76.0
 86.5
 68.9
 -
 91.8
55.4
 58.7
 68.0
 40.3
 -
 63.7
Table 4 | Comparison between DeepSeek-R1 and other representative models.
For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA
Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im-
provement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-
icant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1
excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis
capabilities. This highlights the potential of reasoning models in AI-driven search and data
analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,
demonstrating its capability in handling fact-based queries. A similar trend is observed where
OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than
DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse
answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an
accuracy of over 70%.
DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a
model’s ability to follow format instructions. These improvements can be linked to the inclusion
of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL
training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,
indicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its
significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale
RL, which not only boosts reasoning capabilities but also improves performance across diverse
domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an
average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that
13
DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying
its robustness across multiple tasks.
On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,
surpassing other models by a large margin. A similar trend is observed on coding algorithm
tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these
benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1
on Aider but achieves comparable performance on SWE Verified. We believe the engineering
performance of DeepSeek-R1 will improve in the next version, as the amount of related RL
training data currently remains very limited.
3.2. Distilled Model Evaluation
Model
GPQA
 LiveCode
AIME 2024
 MATH-500
 CodeForces
Diamond
 Bench
pass@1
 cons@64
 pass@1
 pass@1
 pass@1
 rating
GPT-4o-0513
 9.3
 13.4
 74.6
 49.9
 32.9
 759
Claude-3.5-Sonnet-1022
 16.0
 26.7
 78.3
 65.0
 38.9
 717
OpenAI-o1-mini
 63.6
 80.0
 90.0
 60.0
 53.8
 1820
QwQ-32B-Preview
 50.0
 60.0
 90.6
 54.5
 41.9
 1316
DeepSeek-R1-Distill-Qwen-1.5B
 28.9
 52.7
 83.9
 33.8
 16.9
 954
DeepSeek-R1-Distill-Qwen-7B
 55.5
 83.3
 92.8
 49.1
 37.6
 1189
DeepSeek-R1-Distill-Qwen-14B
 69.7
 80.0
 93.9
 59.1
 53.1
 1481
DeepSeek-R1-Distill-Qwen-32B
 72.6
 83.3
 94.3
 62.1
 57.2
 1691
DeepSeek-R1-Distill-Llama-8B
 50.4
 80.0
 89.1
 49.0
 39.6
 1205
DeepSeek-R1-Distill-Llama-70B
 70.0
 86.7
 94.5
 65.2
 57.5
 1633
Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on
reasoning-related benchmarks.
As shown in Table 5, simply distilling DeepSeek-R1’s outputs enables the efficient DeepSeek-
R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-
reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-
Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly
exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla-
tion. Additionally, we found that applying RL to these distilled models yields significant further
gains. We believe this warrants further exploration and therefore present only the results of the
simple SFT-distilled models here.