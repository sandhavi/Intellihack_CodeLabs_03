[
  {
    "question": "What is the purpose of DeepSeek's open-source initiative?",
    "answer": "DeepSeek's open-source initiative aims to share production-ready tools and frameworks to accelerate AGI exploration. By open-sourcing their code, DeepSeek fosters collaboration, transparency, and innovation within the AI community."
  },
  {
    "question": "What is FlashMLA, and how is it optimized for Hopper GPUs?",
    "answer": "FlashMLA is an efficient Multi-Head Latent Attention (MLA) decoding kernel optimized for Hopper GPUs. It supports BF16, uses a paged KV cache with a block size of 64, and achieves 3000 GB/s memory-bound performance and 580 TFLOPS compute-bound performance on H800 GPUs."
  },
  {
    "question": "What is DeepEP, and what are its key features?",
    "answer": "DeepEP is an open-source Expert Parallelism (EP) communication library for MoE (Mixture of Experts) model training and inference. It provides efficient all-to-all communication, supports intranode and internode communication with NVLink and RDMA, and offers high-throughput and low-latency kernels for training and inference."
  },
  {
    "question": "What is DeepGEMM, and why is it significant for DeepSeek-V3/R1?",
    "answer": "DeepGEMM is an FP8 GEMM (General Matrix Multiply) library that supports both dense and MoE GEMMs. It powers DeepSeek-V3/R1 training and inference, delivering up to 1350+ FP8 TFLOPS on Hopper GPUs. It is lightweight, Just-In-Time compiled, and outperforms expert-tuned kernels across most matrix sizes."
  },
  {
    "question": "What is DualPipe, and how does it improve training efficiency?",
    "answer": "DualPipe is a bidirectional pipeline parallelism algorithm designed to overlap computation and communication during DeepSeek-V3/R1 training. It reduces pipeline bubbles and improves GPU utilization, especially on weaker GPUs like the H800."
  },
  {
    "question": "What is EPLB, and what is its role in DeepSeek-V3/R1?",
    "answer": "EPLB (Expert Parallelism Load Balancer) is a load balancing algorithm for DeepSeek-V3/R1. It ensures balanced GPU loads by duplicating heavy-loaded experts and heuristically packing them to GPUs, optimizing performance during training and inference."
  },
  {
    "question": "What is 3FS, and what are its key features?",
    "answer": "3FS (Fire-Flyer File System) is a high-performance distributed file system designed for AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide strong consistency, high throughput (6.6 TiB/s aggregate read throughput), and support for diverse workloads like training data preprocessing, checkpointing, and KVCache lookups."
  },
  {
    "question": "What is Smallpond, and how does it relate to 3FS?",
    "answer": "Smallpond is a data processing framework built on top of 3FS. It enables efficient data preprocessing, dataset loading, and other data-intensive tasks, leveraging the high throughput and strong consistency of 3FS."
  },
  {
    "question": "What are the performance metrics of DeepSeek-V3/R1 inference system?",
    "answer": "The DeepSeek-V3/R1 inference system achieves 73.7k input tokens and 14.8k output tokens per second per H800 node, with a cost profit margin of 545%. It optimizes throughput and latency through cross-node EP-powered batch scaling, computation-communication overlap, and load balancing."
  },
  {
    "question": "What is the significance of DeepSeek's production data for V3/R1 online services?",
    "answer": "DeepSeek's production data for V3/R1 online services demonstrates its high efficiency, with 73.7k input tokens and 14.8k output tokens processed per second per H800 node. The cost profit margin of 545% highlights its economic viability and performance optimization."
  },
  {
    "question": "What is the role of FP8 in DeepGEMM and DeepEP?",
    "answer": "FP8 (8-bit floating point) is used in DeepGEMM and DeepEP to improve computational efficiency and reduce memory usage. It enables high-performance matrix operations and communication, making it ideal for training and inference in large-scale AI models like DeepSeek-V3/R1."
  },
  {
    "question": "What is the GraySort benchmark, and how does 3FS perform on it?",
    "answer": "The GraySort benchmark measures sort performance on large-scale datasets. 3FS achieved a throughput of 3.66 TiB/min on a 25-node cluster, demonstrating its high performance and efficiency in data-intensive tasks."
  },
  {
    "question": "What is the role of KVCache in DeepSeek-V3/R1 inference?",
    "answer": "KVCache (Key-Value Cache) optimizes LLM inference by caching key and value vectors of previous tokens, avoiding redundant computations. 3FS supports KVCache with a peak throughput of 40+ GiB/s per client node, enhancing inference efficiency."
  },
  {
    "question": "What are the advantages of 3FS's disaggregated architecture?",
    "answer": "3FS's disaggregated architecture combines the throughput of thousands of SSDs and the bandwidth of hundreds of storage nodes, enabling applications to access storage resources in a locality-oblivious manner. This design simplifies distributed application development and ensures high performance."
  },
  {
    "question": "How does DeepSeek ensure strong consistency in 3FS?",
    "answer": "3FS ensures strong consistency by implementing Chain Replication with Apportioned Queries (CRAQ). This protocol guarantees that write operations are propagated across all replicas before being acknowledged, while read operations can be served by any replica, ensuring data integrity and high availability."
  },
  {
    "question": "What is the role of computation-communication overlap in DeepSeek-V3/R1?",
    "answer": "Computation-communication overlap in DeepSeek-V3/R1 reduces idle time (bubbles) during training by overlapping data transfer and computation tasks. This optimization improves GPU utilization and overall training efficiency, especially on weaker GPUs like the H800."
  },
  {
    "question": "What is the significance of DeepSeek's open-source initiative for the AI community?",
    "answer": "DeepSeek's open-source initiative provides the AI community with production-ready tools and frameworks, enabling developers to build on their work and contribute to collective progress. It fosters collaboration, transparency, and innovation in AGI exploration."
  },
  {
    "question": "What is the role of the cluster manager in 3FS?",
    "answer": "The cluster manager in 3FS handles membership changes, distributes cluster configuration, and ensures high availability by electing a primary manager. It monitors heartbeats from metadata and storage services to detect failures and maintain system integrity."
  },
  {
    "question": "What is the role of the metadata service in 3FS?",
    "answer": "The metadata service in 3FS handles file system semantics (e.g., open/create files/directories) and stores file metadata in a transactional key-value store like FoundationDB. It is stateless, allowing seamless upgrades and restarts without disruption."
  },
  {
    "question": "What is the role of the storage service in 3FS?",
    "answer": "The storage service in 3FS manages local SSDs and provides a chunk store interface. It implements Chain Replication with Apportioned Queries (CRAQ) to ensure strong consistency and high throughput for file data."
  },
  {
    "question": "What are the limitations of FUSE in 3FS?",
    "answer": "FUSE has performance limitations, including memory copy overhead between kernel and user spaces, and poor multi-threading support due to lock contention. It also lacks concurrent write support to the same file in Linux 5.x."
  },
  {
    "question": "What is the asynchronous zero-copy API in 3FS?",
    "answer": "The asynchronous zero-copy API in 3FS allows applications to perform I/O operations without memory copying overhead. It uses shared memory regions (Iov) and ring buffers (Ior) for efficient communication between user processes and the native client."
  },
  {
    "question": "What is the role of the chunk allocator in 3FS?",
    "answer": "The chunk allocator in 3FS manages the allocation of physical blocks for storing chunk data. It uses a resource pool with bitmaps to track block usage and ensures efficient allocation and deallocation of storage space."
  },
  {
    "question": "What is the significance of DeepSeek's production data for V3/R1 online services?",
    "answer": "DeepSeek's production data for V3/R1 online services demonstrates its high efficiency, with 73.7k input tokens and 14.8k output tokens processed per second per H800 node. The cost profit margin of 545% highlights its economic viability and performance optimization."
  }
]