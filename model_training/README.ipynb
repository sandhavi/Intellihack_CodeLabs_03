{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genenerate Dataset\n",
    "\n",
    "Using the deepseek, conver the extracted text preserving the chapters content as exact will be send to generate Q&A pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install requests dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_1.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_1.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_10.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_10.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_11.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_11.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_2.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_2.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_3.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_3.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_4.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_4.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_5.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_5.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_6.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_6.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_7.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_7.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_8.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_8.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_9.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_9.txt\n",
      "Waiting for 10 seconds before the next file...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Replace with your DeepSeek API key\n",
    "API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "# DeepSeek API endpoint\n",
    "API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "\n",
    "# Input and output directories\n",
    "INPUT_DIR = \"./data/text_extraction_data\"\n",
    "OUTPUT_DIR = \"./data/json_dataset\"\n",
    "\n",
    "# Wait time between API calls (in seconds)\n",
    "WAIT_TIME = 10  # Adjust based on API rate limits\n",
    "\n",
    "# Function to generate Q&A pairs\n",
    "def generate_qa_pairs(text):\n",
    "    # Define the prompt for the API\n",
    "    prompt = f\"\"\"\n",
    "    Generate question-answer pairs in JSON format from the following text. \n",
    "    Ensure the questions are clear and the answers are concise and accurate.\n",
    "    The output should be a list of dictionaries, where each dictionary has two keys: \"question\" and \"answer\".\n",
    "\n",
    "    Example Output:\n",
    "    [\n",
    "      {{\n",
    "        \"question\": \"What is the purpose of DeepSeek's open-source initiative?\",\n",
    "        \"answer\": \"DeepSeek's open-source initiative aims to share production-ready tools and frameworks to accelerate AGI exploration. By open-sourcing their code, DeepSeek fosters collaboration, transparency, and innovation within the AI community.\"\n",
    "      }},\n",
    "      {{\n",
    "        \"question\": \"What is FlashMLA, and how is it optimized for Hopper GPUs?\",\n",
    "        \"answer\": \"FlashMLA is an efficient Multi-Head Latent Attention (MLA) decoding kernel optimized for Hopper GPUs. It supports BF16, uses a paged KV cache with a block size of 64, and achieves 3000 GB/s memory-bound performance and 580 TFLOPS compute-bound performance on H800 GPUs.\"\n",
    "      }}\n",
    "    ]\n",
    "\n",
    "    Text:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare the request payload\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-chat\",  # Specify the model\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates question-answer pairs in JSON format.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 2000,  # Adjust based on the length of the text\n",
    "        \"temperature\": 0.7,  # Controls creativity (0.7 is a good balance)\n",
    "    }\n",
    "\n",
    "    # Set up headers with the API key\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Send the request to the DeepSeek API\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response\n",
    "        response_data = response.json()\n",
    "        qa_pairs = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        # Convert the response to a Python list of dictionaries\n",
    "        try:\n",
    "            qa_pairs = json.loads(qa_pairs)\n",
    "            return qa_pairs\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error: The response is not valid JSON.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Function to process all input files\n",
    "def process_files(input_dir, output_dir, wait_time):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Get a list of all .txt files in the input directory\n",
    "    input_files = [f for f in os.listdir(input_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "    # Process each file sequentially\n",
    "    for file_name in input_files:\n",
    "        # Construct full file paths\n",
    "        input_path = os.path.join(input_dir, file_name)\n",
    "        output_path = os.path.join(output_dir, file_name.replace(\".txt\", \".json\"))\n",
    "\n",
    "        # Read the input file\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Generate Q&A pairs\n",
    "        print(f\"Processing {file_name}...\")\n",
    "        qa_pairs = generate_qa_pairs(text)\n",
    "\n",
    "        # Save the result as a JSON file\n",
    "        if qa_pairs:\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(qa_pairs, f, indent=4)\n",
    "            print(f\"Saved Q&A pairs to {output_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to generate Q&A pairs for {file_name}\")\n",
    "\n",
    "        # Wait before processing the next file\n",
    "        print(f\"Waiting for {wait_time} seconds before the next file...\")\n",
    "        time.sleep(wait_time)\n",
    "\n",
    "# Run the script\n",
    "process_files(INPUT_DIR, OUTPUT_DIR, WAIT_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all json data to single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added data_1.json to the combined dataset.\n",
      "Added data_10.json to the combined dataset.\n",
      "Added data_11.json to the combined dataset.\n",
      "Added data_12.json to the combined dataset.\n",
      "Added data_13.json to the combined dataset.\n",
      "Added data_14.json to the combined dataset.\n",
      "Added data_15.json to the combined dataset.\n",
      "Added data_2.json to the combined dataset.\n",
      "Added data_3.json to the combined dataset.\n",
      "Added data_4.json to the combined dataset.\n",
      "Added data_5.json to the combined dataset.\n",
      "Added data_6.json to the combined dataset.\n",
      "Added data_7.json to the combined dataset.\n",
      "Added data_8.json to the combined dataset.\n",
      "Added data_9.json to the combined dataset.\n",
      "Combined dataset saved to ./data/dataset.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Output directory containing individual JSON files\n",
    "OUTPUT_DIR = \"./data/json_dataset\"\n",
    "\n",
    "# Output file for the combined dataset\n",
    "COMBINED_OUTPUT_FILE = \"./data/dataset.json\"\n",
    "\n",
    "# Function to combine JSON files\n",
    "def combine_json_files(output_dir, combined_output_file):\n",
    "    # Initialize an empty list to store all Q&A pairs\n",
    "    combined_data = []\n",
    "\n",
    "    # Get a list of all .json files in the output directory\n",
    "    json_files = [f for f in os.listdir(output_dir) if f.endswith(\".json\")]\n",
    "\n",
    "    # Process each JSON file\n",
    "    for file_name in json_files:\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "        # Read the JSON file\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Append the data to the combined list\n",
    "        combined_data.extend(data)\n",
    "        print(f\"Added {file_name} to the combined dataset.\")\n",
    "\n",
    "    # Save the combined data to a single JSON file\n",
    "    with open(combined_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(combined_data, f, indent=4)\n",
    "    print(f\"Combined dataset saved to {combined_output_file}\")\n",
    "\n",
    "# Run the script\n",
    "combine_json_files(OUTPUT_DIR, COMBINED_OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fien-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "def finetune_qwen_colab_lightweight(train_file, model_name=\"Qwen/Qwen2.5-3B-Instruct\", output_dir=\"./qwen2.5-3b-research-qa-lora\"):\n",
    "    \"\"\"\n",
    "    Fine-tunes a Qwen model using a custom dataset.\n",
    "    Optimized for minimal memory usage in Google Colab to avoid OutOfMemoryError.\n",
    "\n",
    "    Key memory reduction strategies applied:\n",
    "    - Reduced per_device_train_batch_size to 1\n",
    "    - Kept gradient_accumulation_steps to 4 (effective batch size 4)\n",
    "    - Reduced max_length for tokenization to 256\n",
    "    - Using 4-bit quantization (load_in_4bit=True)\n",
    "    - CPU Offloading enabled\n",
    "\n",
    "    Args:\n",
    "        train_file (str): Path to the JSON file containing the training data (in Colab environment).\n",
    "        model_name (str): Name of the Qwen model to fine-tune.\n",
    "        output_dir (str): Output directory for saving the fine-tuned LoRA adapters (in Colab environment).\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Google Colab Specific Setup (Installation - Run this in Colab) ---\n",
    "    # print(\"Installing required libraries in Colab...\")\n",
    "    # !pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "\n",
    "    # --- Clear GPU Cache ---\n",
    "    print(\"Clearing GPU memory cache...\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # --- 1. Load Model and Tokenizer ---\n",
    "    print(f\"Loading tokenizer and model: {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        offload_folder=\"offload\",\n",
    "        offload_state_dict=True,\n",
    "        # Remove load_in_4bit=True  <--- REMOVE THIS LINE\n",
    "        quantization_config={\"load_in_4bit\": True} # KEEP quantization_config\n",
    "    )\n",
    "\n",
    "    # --- 2. Prepare Model for QLoRA ---\n",
    "    print(\"Preparing model for QLoRA...\")\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    # --- 3. Load and Tokenize Dataset ---\n",
    "    print(f\"Loading and tokenizing dataset from: {train_file}...\")\n",
    "    def preprocess_function(examples):\n",
    "        inputs = [f\"Question: {q} Answer: \" for q in examples[\"question\"]]\n",
    "        targets = [a for a in examples[\"answer\"]]\n",
    "        model_inputs = tokenizer(inputs, text_target=targets, max_length=256, truncation=True, # Reduced max_length to 256\n",
    "                                   padding=\"max_length\")\n",
    "        return model_inputs\n",
    "\n",
    "    try:\n",
    "        dataset = load_dataset(\"json\", data_files=train_file, split=\"train\")\n",
    "        tokenized_train_dataset = dataset.map(preprocess_function, batched=True)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Training data file not found at {train_file}. Make sure to upload it to Colab.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error: An error occurred reading the JSON file: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 4. Set up Training Arguments ---\n",
    "    print(\"Setting up training arguments (Lightweight Config)...\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=1, # **Reduced to 1 for minimal memory**\n",
    "        gradient_accumulation_steps=4, # Effective batch size = 4 (still reasonable)\n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=50,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        evaluation_strategy=\"no\",\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.05,\n",
    "        report_to=\"none\",\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    # --- 5. Create Trainer and Train ---\n",
    "    print(\"Initializing Trainer and starting training...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # --- 6. Save Trained Model (LoRA Adapters) ---\n",
    "    print(\"Saving trained LoRA adapters...\")\n",
    "    model.save_pretrained(output_dir)\n",
    "    print(f\"Fine-tuning complete! LoRA adapters saved to {output_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ---  Instructions for Google Colab Users (LIGHTWEIGHT VERSION) ---\n",
    "    print(\"\\n--- Instructions for Google Colab (LIGHTWEIGHT VERSION) ---\")\n",
    "    print(\"1. **Upload Dataset:** Upload your `dataset.json` file to the Colab environment.\")\n",
    "    print(\"   You can do this by dragging and dropping it into the Files sidebar (left side in Colab).\")\n",
    "    print(\"2. **Set `train_data_file` Path:** Ensure `train_data_file` below points to the correct path\")\n",
    "    print(\"   where you uploaded `dataset.json` in Colab.  For example: `'dataset.json'` or `'./data/dataset.json'`\")\n",
    "    print(\"3. **Run the Code:** Execute this Python code cell in Colab.\")\n",
    "    print(\"4. **Check Output:** After training, LoRA adapters will be in `qwen2.5-3b-research-qa-lora` folder.\")\n",
    "    print(\"   Download this folder from Colab's Files sidebar.\")\n",
    "    print(\"---\")\n",
    "    print(\"\\n**This version is optimized for minimal memory usage in Colab.**\")\n",
    "    print(\"**If you still get OutOfMemoryError, consider further reducing `max_length` to 128 in the code.**\")\n",
    "    print(\"---\")\n",
    "\n",
    "    train_data_file = \"./data/dataset.json\"  # Path to your JSON training data file in Colab\n",
    "    finetune_qwen_colab_lightweight(train_data_file)  # Run the lightweight fine-tuning function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the gguf File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "cd llama.cpp\n",
    "make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below command to generate the .gguf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "py ./convert_lora_to_gguf.py ../qwen2.5-3b-research-qa-lora --outfile ../qwen2.5-3b-research-qa-lora.gguf --base-model-id Qwen/Qwen2.5-3B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Chatbot with In-Memory Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: redis in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (5.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch llama-cpp-python redis datasets peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redis Server:\n",
    "\n",
    "- Linux: `sudo apt install redis-server`\n",
    "\n",
    "- macOS: `brew install redis`\n",
    "\n",
    "- Windows: Download from [Microsoft's Redis repo](https://github.com/microsoftarchive/redis/releases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import redis\n",
    "\n",
    "# Load the fine-tuned .gguf model\n",
    "model_path = \"./qwen2.5-3b-research-qa-lora.gguf\"  # Replace with the correct path\n",
    "llm = Llama(model_path=model_path, n_ctx=2048)  # Increase context size if needed\n",
    "\n",
    "# Initialize Redis for chat history\n",
    "redis_client = redis.Redis(host=\"localhost\", port=6379, db=0)\n",
    "\n",
    "# Function to generate a response\n",
    "def generate_response(prompt, chat_history):\n",
    "    # Limit chat history length (avoid excessive token usage)\n",
    "    max_history_length = 10  # Keep last 10 exchanges\n",
    "    chat_history = chat_history[-max_history_length:]\n",
    "\n",
    "    # Combine chat history with the new prompt\n",
    "    full_prompt = \"\\n\".join(chat_history + [f\"User: {prompt}\"]) + \"\\nChatbot:\"\n",
    "\n",
    "    # Generate response\n",
    "    output = llm.create_completion(\n",
    "        prompt=full_prompt,\n",
    "        max_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        stop=[\"User:\", \"\\nUser:\"],  # Ensure bot stops before next user input\n",
    "        echo=False\n",
    "    )\n",
    "\n",
    "    return output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# Chat loop\n",
    "def chat():\n",
    "    chat_id = \"user_123\"  # Unique ID for the chat session\n",
    "    chat_history = redis_client.lrange(chat_id, 0, -1)  # Load chat history\n",
    "    chat_history = [msg.decode(\"utf-8\") for msg in chat_history]\n",
    "\n",
    "    print(\"Chatbot: Hello! How can I assist you today?\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Generate response\n",
    "        response = generate_response(user_input, chat_history)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "\n",
    "        # Update chat history (keeping only recent interactions)\n",
    "        chat_history.append(f\"User: {user_input}\")\n",
    "        chat_history.append(f\"Chatbot: {response}\")\n",
    "        redis_client.rpush(chat_id, f\"User: {user_input}\", f\"Chatbot: {response}\")\n",
    "\n",
    "        # Trim Redis history to avoid excessive memory usage\n",
    "        redis_client.ltrim(chat_id, -20, -1)  # Keep last 20 messages\n",
    "\n",
    "# Start the chat\n",
    "if __name__ == \"__main__\":\n",
    "    chat()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
